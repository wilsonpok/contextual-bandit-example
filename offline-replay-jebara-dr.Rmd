---
title: 'Offline replay - doubly robust jebara'
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(contextual)
library(knitr)
library(tidyverse)
library(ggthemes)
library(brglm)
library(parallel)
```


```{r functions}
model_f_dm <- function(arm){
  brglm(f_dm1, data = data %>% filter(choice == arm))
  }

predict_arm <- function(model){
  predict(model, data, type = "response")
	}
```

## Introduction
Here, we will use doubly robust offline evaluation to compare various bandit policies (A/B testing, MAB, CB). The dataset used here is from the ["Machine Learning for Personalization" course](http://www.cs.columbia.edu/~jebara/6998/)

### Data
The raw data contains 10k rows with a choice (from 10 options), a binary reward, and 100 context features.
```{r load_data}
url <- "http://d1ie9wlkzugsxr.cloudfront.net/data_cmab_basic/dataset.txt"
names <- c('choice', 'reward', str_c('x', seq(1:100)), 'empty_col')
raw_data <- read_delim(url, delim = ' ', col_names = names) %>% select(-empty_col)
```

```{r plot_raw_data}
results1 <- raw_data %>%
	group_by(choice) %>%
	summarise(n = n(),
		sum_reward = sum(reward),
		mean_reward = mean(reward),
		error_min = qbeta(0.025, 1 + sum_reward, 1 + n - sum_reward),
		error_max = qbeta(0.975, 1 + sum_reward, 1 + n - sum_reward),
		.groups = 'drop')

results1 %>%
 mutate(choice = choice %>% as.factor()) %>%
  ggplot(aes(x = choice, y = mean_reward)) +
  geom_point() +
  geom_errorbar(aes(ymin = error_min, ymax = error_max), width = 0.1) +
  theme_fivethirtyeight()

raw_data %>% 
  group_by(choice) %>% 
  summarise(n = n(), .groups = 'drop') %>% 
  kable(format = 'markdown')

raw_data %>% 
  group_by(reward) %>% 
  summarise(n = n(), .groups = 'drop') %>% 
  kable(format = 'markdown')
```

However, in this example, we will only consider observations involving arms 6-10 (4972 rows), which we rename as arms 1-5.
```{r}
data <- raw_data %>%
  filter(choice %in% seq(6, 10)) %>% 
  mutate(choice = choice - 5) %>% 
  mutate(t = 1:n())

data %>% dim()

arms <- sort(unique(data$choice))
```

```{r plot_data}
results <- data %>%
	group_by(choice) %>%
	summarise(n = n(),
		sum_reward = sum(reward),
		mean_reward = mean(reward),
		error_min = qbeta(0.025, 1 + sum_reward, 1 + n - sum_reward),
		error_max = qbeta(0.975, 1 + sum_reward, 1 + n - sum_reward),
		.groups = 'drop')

results %>%
 mutate(choice = choice %>% as.factor()) %>%
  ggplot(aes(x = choice, y = mean_reward)) +
  geom_point() +
  geom_errorbar(aes(ymin = error_min, ymax = error_max), width = 0.1) +
  theme_fivethirtyeight()

data %>% 
  group_by(choice) %>% 
  summarise(n = n(), .groups = 'drop') %>% 
  kable(format = 'markdown')

data %>% 
  group_by(reward) %>% 
  summarise(n = n(), .groups = 'drop') %>% 
  kable(format = 'markdown')
```


## Required inputs
We will replay the `EpsilonFirstPolicy` (A/B testing), `EpsilonGreedyPolicy` (MAB)
, and `LinUCBDisjointPolicy` (CB) policies over this data using the `OfflineDoublyRobustBandit`.

However, the `OfflineDoublyRobustBandit` requires:

1. Fitting a model to the data, as in the direct method
2. Propensities per arm, as in the inverse propensity score method. The Doubly Robust Bandit uses the marginal probabilities per arm if these are omitted.

### Fit DM model
We fit a logistic regression to each arm, and use it to produce a predicted reward for each observation for each arm.
```{r fit_dm_model, include=FALSE}
context_cols <- data %>% select(starts_with('x')) %>% names()

f_dm1 <- as.formula(paste("reward ~", paste(context_cols, collapse = "+")))

model_arms <- arms %>% map(model_f_dm)

r_data <- model_arms %>% map(predict_arm) %>% bind_cols()

colnames(r_data) <- paste0("r", (1:max(arms)))

data_dm <- data %>% bind_cols(r_data)
```

```{r}
data_dm %>% select(choice, starts_with('r')) %>% head()
```

### Fit IPS probabilities
We could fit a model to predict arm probabilities, but for now we'll just default to the marginal probabilities.

## Doubly robust evaluation
```{r}
data_dr <- data_dm

f_dr <- as.formula(paste("reward ~ choice | ", 
                         paste(context_cols, collapse = " + "), " | ",
                         paste(colnames(r_data), collapse = " + ")))

bandit_dr <- OfflineDoublyRobustBandit$new(formula = f_dr, data = data_dr)
```

```{r}
simulations <- 50
horizon <- nrow(data_dr)
```

## Evaluation policy
The policies to be evaluated here are `EpsilonFirstPolicy`, `EpsilonGreedyPolicy`, and `LinUCBDisjointPolicy`.

```{r policy, echo=TRUE}
agents <-list(
	Agent$new(EpsilonFirstPolicy$new(epsilon = 0.1, N = horizon), bandit_dr),
	Agent$new(EpsilonGreedyPolicy$new(epsilon = 0.1), bandit_dr),
	Agent$new(LinUCBDisjointPolicy$new(0.01), bandit_dr)
	)
```

```{r sim_setup}
simulation <- Simulator$new(agents = agents, 
                            simulations = simulations,
                            horizon = horizon, 
                            save_context = TRUE, 
                            set_seed = 666)
```

```{r sim_run, include=FALSE}
history <- simulation$run()
```

## Results
```{r}
history %>% summary()
history_df <- history$get_data_table() %>% as_tibble()
```

## Plots
### Arm choices
```{r}
plot(history, type = 'arms', limit_agents = list('EpsilonFirst'))
plot(history, type = 'arms', limit_agents = list('EpsilonGreedy'))
plot(history, type = 'arms', limit_agents = list('LinUCBDisjoint'))
```

```{r}
agg_choices <- history_df %>% 
  mutate(choice = choice %>% as.factor()) %>% 
  group_by(t, choice, agent) %>% 
  summarise(n_choice = n(), propensity = mean(propensity)) %>% 
  group_by(t, agent) %>% 
  arrange(t, choice) %>%
  mutate(n_total = sum(n_choice), prop_choice = n_choice / n_total) 

agg_choices %>%   
  ggplot(aes(x = t, y = prop_choice, colour = choice)) +
  geom_line() +
  facet_wrap(~ agent, ncol = 1) +
  theme_fivethirtyeight()
```

### Cumulative reward
```{r}
plot(history, type = "cumulative",
	rate = TRUE, regret = FALSE, legend_position = "bottomright", disp = "ci")
```

## Sanity checks
```{r, echo=TRUE}
history_df %>%
  filter(t == 1) %>% 
  group_by(agent) %>% 
  summarise(n = n()) %>% 
  kable()
```

```{r, echo=TRUE}
history_df %>% glimpse()
```

