---  
title: 'Contextual bandit example - run simulation'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(tidyverse)
library(ggthemes)
library(contextual)
library(brglm)
library(parallel)
```

```{r load_data, include=FALSE}
data_file <- '../data.csv'
data <- data_file %>% read_csv()
arms <- sort(unique(data$choice))
```

```{r define_functions}
model_f_dm <- function(arm){
  brglm(f_dm1, data = data %>% filter(choice == arm))
  }

predict_arm <- function(model){
  predict(model, data, type = "response")
	}
```


## Required inputs
We will replay the `EpsilonFirstPolicy` (A/B testing), `EpsilonGreedyPolicy` (MAB)
, and `LinUCBDisjointPolicy` (CB) policies over this data using the `OfflineDoublyRobustBandit`.

However, the `OfflineDoublyRobustBandit` requires:

1. Fitting a model to the data, as in the direct method
2. Propensities per arm, as in the inverse propensity score method. The Doubly Robust Bandit uses the marginal probabilities per arm if these are omitted.

### Fit DM model
We fit a logistic regression to each arm, and use it to produce a predicted reward for each observation for each arm.


```{r fit_dm_model, include=FALSE}
context_cols <- data %>% select(starts_with('x')) %>% names()

f_dm1 <- as.formula(paste("reward ~", paste(context_cols, collapse = "+")))

model_arms <- arms %>% map(model_f_dm)

r_data <- model_arms %>% map(predict_arm) %>% bind_cols()

colnames(r_data) <- paste0("r", (1:max(arms)))

data_dm <- data %>% bind_cols(r_data)
```

```{r}
data_dm %>% select(choice, starts_with('r')) %>% head()
```


### Fit IPS probabilities
We could fit a model to predict arm probabilities, but for now we'll just default to the marginal probabilities.

## Doubly robust evaluation
```{r}
data_dr <- data_dm

f_dr <- as.formula(paste("reward ~ choice | ", 
                         paste(context_cols, collapse = " + "), " | ",
                         paste(colnames(r_data), collapse = " + ")))

bandit_dr <- OfflineDoublyRobustBandit$new(formula = f_dr, data = data_dr)
```

```{r}
simulations <- 50
horizon <- nrow(data_dr)
```

## Evaluation policy
The policies to be evaluated here are `EpsilonFirstPolicy`, `EpsilonGreedyPolicy`, and `LinUCBDisjointPolicy`.

```{r policy, echo=TRUE}
agents <-list(
	Agent$new(EpsilonFirstPolicy$new(epsilon = 0.1, N = horizon), bandit_dr),
	Agent$new(EpsilonGreedyPolicy$new(epsilon = 0.1), bandit_dr),
	Agent$new(LinUCBDisjointPolicy$new(0.01), bandit_dr)
	)
```

```{r sim_setup}
simulation <- Simulator$new(agents = agents, 
                            simulations = simulations,
                            horizon = horizon, 
                            save_context = TRUE, 
                            set_seed = 666)
```

```{r sim_run, include=FALSE}
history <- simulation$run()
```

## Results
```{r}
history %>% summary()
history_df <- history$get_data_table() %>% as_tibble()
```

## Plots
### Arm choices
```{r}
plot(history, type = 'arms', limit_agents = list('EpsilonFirst'))
plot(history, type = 'arms', limit_agents = list('EpsilonGreedy'))
plot(history, type = 'arms', limit_agents = list('LinUCBDisjoint'))
```

```{r}
agg_choices <- history_df %>% 
  mutate(choice = choice %>% as.factor()) %>% 
  group_by(t, choice, agent) %>% 
  summarise(n_choice = n(), propensity = mean(propensity)) %>% 
  group_by(t, agent) %>% 
  arrange(t, choice) %>%
  mutate(n_total = sum(n_choice), prop_choice = n_choice / n_total) 

agg_choices %>%   
  ggplot(aes(x = t, y = prop_choice, colour = choice)) +
  geom_line() +
  facet_wrap(~ agent, ncol = 1) +
  theme_fivethirtyeight()
```

### Cumulative reward
```{r}
plot(history, type = "cumulative",
	rate = TRUE, regret = FALSE, legend_position = "bottomright", disp = "ci")
```

